{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (3.9.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (24.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Requirement already satisfied: pandas in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: pyarrow in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (16.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from pyarrow) (1.26.4)\r\n",
      "Requirement already satisfied: wandb in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (0.17.0)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (4.25.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (5.9.8)\r\n",
      "Requirement already satisfied: pyyaml in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (2.32.2)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (2.3.1)\r\n",
      "Requirement already satisfied: setproctitle in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from wandb) (70.0.0)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romanzberg/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow\n",
    "!pip install wandb\n",
    "\n",
    "import ssl\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'init_lr': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 25,\n",
    "    'empty_image_representation': 'zero_matrix',  #  zero_matrix, torch_empty\n",
    "    'dino_architecture': 'small',\n",
    "    'dataset': 'polyvore_63eb50dc58d97415384467bef7b3c9e1bd6c96e06ad19571b6bc15e9dd5af262.parquet',\n",
    "    'model_forward_version': 'fast',  # slow, fast\n",
    "    'hidden_layer_neuron_count': 64,\n",
    "    'dropout_probability': 0.1,\n",
    "    'regularisation': 'l1',\n",
    "    'regularisation_weight': 1\n",
    "}\n",
    "\n",
    "dataset_folder_root_path = '../datasets'\n",
    "dataset_path = f'{dataset_folder_root_path}/imageBasedModel/polyvore/{model_config[\"dataset\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': Compose(\n       PILToTensor()\n       Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n       CenterCrop(size=(224, 224))\n       RandomHorizontalFlip(p=0.5)\n       RandomPerspective(p=0.5, distortion_scale=0.5, interpolation=InterpolationMode.BILINEAR, fill=255)\n       RandomAffine(degrees=[-30.0, 30.0], interpolation=InterpolationMode.NEAREST, fill=255)\n       ConvertImageDtype()\n       Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n ),\n 'val': Compose(\n       PILToTensor()\n       Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n       CenterCrop(size=(224, 224))\n       ConvertImageDtype()\n       Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n ),\n 'test': Compose(\n       PILToTensor()\n       Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n       CenterCrop(size=(224, 224))\n       ConvertImageDtype()\n       Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n )}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': v2.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomPerspective(fill=255),\n",
    "        v2.RandomAffine(30, fill=255),\n",
    "        # v2.AutoAugment(v2.AutoAugmentPolicy.IMAGENET),\n",
    "        v2.ConvertImageDtype(torch.float32),\n",
    "        v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    'val': v2.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.ConvertImageDtype(torch.float32),\n",
    "        v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    'test': v2.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.ConvertImageDtype(torch.float32),\n",
    "        v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_transforms\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fix_random_seeds(seed=12345):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "fix_random_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load feature extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/jovyan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/jovyan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/jovyan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "backbone_archs = {\n",
    "    'small': 'vits14',\n",
    "    'base': 'vitb14',\n",
    "    'large': 'vitl14',\n",
    "    'giant': 'vitg14',\n",
    "}\n",
    "\n",
    "backbone_arch = backbone_archs[model_config['dino_architecture']]\n",
    "backbone_name = f'dinov2_{backbone_arch}'\n",
    "feature_extraction_model = torch.hub.load('facebookresearch/dinov2', backbone_name).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extraction_model.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# freeze weights of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for param in feature_extraction_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get ImageNet labels\n",
    "imagenet_class_url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "imagenet_classes = json.loads(urllib.request.urlopen(imagenet_class_url).read())\n",
    "\n",
    "\n",
    "# # Set a device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#https://www.modelbit.com/blog/deploying-dinov2-for-image-classification-with-modelbit#:~:text=To%20do%20this%2C%20simply%20use,machine%20to%20your%20Colab%20directory.&text=Next%2C%20you'll%20want%20to,ImageNet%20preprocessing%20on%20the%20image.&text=Now%2C%20we%20can%20pass%20the,a%20class%20ID%20and%20label.\n",
    "def dinov2_classifier(img_url):\n",
    "    response = requests.get(img_url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Preprocess the image\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "\n",
    "    # Move the image to the GPU if available\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Extract the features\n",
    "    with torch.no_grad():\n",
    "        features = torch.squeeze(feature_extraction_model(image.unsqueeze(0)))\n",
    "        # print(features)\n",
    "        # print(features.shape)\n",
    "\n",
    "    # Print the features\n",
    "    return {'index': features.argmax(-1).item(),\n",
    "            'label': imagenet_classes[features.argmax(-1).item()]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.288867712020874 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "dinov2_classifier(\n",
    "    \"https://www.apple.com/v/iphone/home/bu/images/meta/iphone__ky2k6x5u6vue_og.png\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Implementing Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Class for test dataset\n",
    "def get_image(img_path):\n",
    "    if img_path is not None:\n",
    "        img_path = img_path.replace('raw/images', 'resized/256x256')\n",
    "\n",
    "    if img_path is None:\n",
    "        if model_config[\"empty_image_representation\"] == \"zero_matrix\":\n",
    "            return torch.zeros(3, 224, 224)\n",
    "        elif model_config[\"empty_image_representation\"] == \"torch_empty\":\n",
    "            return torch.empty(3, 224, 224)\n",
    "        else:\n",
    "            raise Exception(\"Wrong configuration value for key empty_image_representation in model_configuration\")\n",
    "    else:\n",
    "        return read_image(f'{dataset_folder_root_path}/{img_path}')\n",
    "\n",
    "\n",
    "class OutfitClassifier(nn.Module):\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self):\n",
    "        return self._embed_dim\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OutfitClassifier, self).__init__()\n",
    "        self._embed_dim = feature_extraction_model.embed_dim * 5\n",
    "\n",
    "        feature_extraction_model.eval().to(device)\n",
    "\n",
    "        hidden_size = model_config['hidden_layer_neuron_count']\n",
    "\n",
    "        self.trainable_model = nn.Sequential(\n",
    "            nn.Linear(self._embed_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(model_config['dropout_probability']),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(model_config['dropout_probability']),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(model_config['dropout_probability']),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.trainable_model.to(device)\n",
    "        self.counter = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        number_of_rows = X.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Reshape X to concatenate along the batch dimension\n",
    "            # New shape will be [5 * batch_size, channels, height, width]\n",
    "            dino_input = X.view(number_of_rows * 5, 3, 224, 224)\n",
    "\n",
    "            batch_features = feature_extraction_model(dino_input)\n",
    "\n",
    "            # dino batch features torch.Size([160, 384])\n",
    "\n",
    "            # Reshape the features to [batch_size, embed_dim]\n",
    "            batch_features = torch.reshape(batch_features, (int(batch_features.shape[0] / 5), self.embed_dim))\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_features.requires_grad_()\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        return self.trainable_model.forward(batch_features)\n",
    "\n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_frame, tfms: v2.Compose, name):\n",
    "        self.df = data_frame\n",
    "        self.tfms = tfms\n",
    "\n",
    "        img_accessoires = []\n",
    "        img_innerwear = []\n",
    "        img_bottomwear = []\n",
    "        img_shoe = []\n",
    "        img_outerwear = []\n",
    "        valid_outfit = []\n",
    "        for index, outfit in tqdm(self.df.iterrows(), total=self.df.shape[0], desc=f'Loading {name} dataset'):\n",
    "            img_accessoires.append(get_image(outfit['Accessoire_imagePath']))\n",
    "            img_innerwear.append(get_image(outfit['Innerwear_imagePath']))\n",
    "            img_bottomwear.append(get_image(outfit['Bottomwear_imagePath']))\n",
    "            img_shoe.append(get_image(outfit['Shoes_imagePath']))\n",
    "            img_outerwear.append(get_image(outfit['Outerwear_imagePath']))\n",
    "            valid_outfit.append(outfit['valid_outfit'])\n",
    "\n",
    "        self.feature_df = pd.DataFrame({\n",
    "            'Accessoire_imagePath': img_accessoires,\n",
    "            'Innerwear_imagePath': img_innerwear,\n",
    "            'Bottomwear_imagePath': img_bottomwear,\n",
    "            'Shoes_imagePath': img_shoe,\n",
    "            'Outerwear_imagePath': img_outerwear,\n",
    "            'valid_outfit': valid_outfit\n",
    "        }, index=self.df.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(index)\n",
    "        #start_time = time.time()\n",
    "        outfit = self.feature_df.iloc[index]\n",
    "        img_accessoire = self.tfms(outfit['Accessoire_imagePath'].to(device))\n",
    "        img_innerwear = self.tfms(outfit['Innerwear_imagePath'].to(device))\n",
    "        img_bottomwear = self.tfms(outfit['Bottomwear_imagePath'].to(device))\n",
    "        img_shoe = self.tfms(outfit['Shoes_imagePath'].to(device))\n",
    "        img_outerwear = self.tfms(outfit['Outerwear_imagePath'].to(device))\n",
    "\n",
    "        target_variable = torch.tensor([outfit['valid_outfit']]).to(torch.float).to(device)\n",
    "\n",
    "        feature_tensor = torch.cat([\n",
    "            img_accessoire.unsqueeze(0),\n",
    "            img_innerwear.unsqueeze(0),\n",
    "            img_bottomwear.unsqueeze(0),\n",
    "            img_shoe.unsqueeze(0),\n",
    "            img_outerwear.unsqueeze(0)\n",
    "        ]).to(device)\n",
    "\n",
    "        #print(f'finished dataset get item {time.time() - start_time}')\n",
    "\n",
    "        return feature_tensor, target_variable\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Innerwear_imagePath</th>\n",
       "      <th>Bottomwear_imagePath</th>\n",
       "      <th>Accessoire_imagePath</th>\n",
       "      <th>Shoes_imagePath</th>\n",
       "      <th>Outerwear_imagePath</th>\n",
       "      <th>valid_outfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120161271</th>\n",
       "      <td>raw/images/120161271/1.jpg</td>\n",
       "      <td>raw/images/120161271/2.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143656996</th>\n",
       "      <td>raw/images/143656996/1.jpg</td>\n",
       "      <td>raw/images/143656996/3.jpg</td>\n",
       "      <td>raw/images/143656996/5.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216470135</th>\n",
       "      <td>raw/images/216470135/1.jpg</td>\n",
       "      <td>raw/images/216470135/2.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/216470135/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216220312</th>\n",
       "      <td>raw/images/216220312/1.jpg</td>\n",
       "      <td>raw/images/216220312/2.jpg</td>\n",
       "      <td>raw/images/216220312/4.jpg</td>\n",
       "      <td>raw/images/216220312/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192203629</th>\n",
       "      <td>raw/images/192203629/2.jpg</td>\n",
       "      <td>raw/images/192203629/3.jpg</td>\n",
       "      <td>raw/images/192203629/5.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/192203629/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201717504</th>\n",
       "      <td>raw/images/201717504/1.jpg</td>\n",
       "      <td>raw/images/201717504/2.jpg</td>\n",
       "      <td>raw/images/201717504/4.jpg</td>\n",
       "      <td>raw/images/201717504/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216589548</th>\n",
       "      <td>raw/images/216589548/1.jpg</td>\n",
       "      <td>raw/images/216589548/3.jpg</td>\n",
       "      <td>raw/images/216589548/5.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/216589548/2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216860218</th>\n",
       "      <td>raw/images/216860218/1.jpg</td>\n",
       "      <td>raw/images/216860218/3.jpg</td>\n",
       "      <td>raw/images/216860218/5.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/216860218/2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211099953</th>\n",
       "      <td>raw/images/211099953/1.jpg</td>\n",
       "      <td>raw/images/211099953/2.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/211099953/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187953065</th>\n",
       "      <td>raw/images/187953065/1.jpg</td>\n",
       "      <td>raw/images/187953065/3.jpg</td>\n",
       "      <td>raw/images/187953065/5.jpg</td>\n",
       "      <td>raw/images/187953065/4.jpg</td>\n",
       "      <td>raw/images/187953065/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6723 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Innerwear_imagePath        Bottomwear_imagePath  \\\n",
       "120161271  raw/images/120161271/1.jpg  raw/images/120161271/2.jpg   \n",
       "143656996  raw/images/143656996/1.jpg  raw/images/143656996/3.jpg   \n",
       "216470135  raw/images/216470135/1.jpg  raw/images/216470135/2.jpg   \n",
       "216220312  raw/images/216220312/1.jpg  raw/images/216220312/2.jpg   \n",
       "192203629  raw/images/192203629/2.jpg  raw/images/192203629/3.jpg   \n",
       "...                               ...                         ...   \n",
       "201717504  raw/images/201717504/1.jpg  raw/images/201717504/2.jpg   \n",
       "216589548  raw/images/216589548/1.jpg  raw/images/216589548/3.jpg   \n",
       "216860218  raw/images/216860218/1.jpg  raw/images/216860218/3.jpg   \n",
       "211099953  raw/images/211099953/1.jpg  raw/images/211099953/2.jpg   \n",
       "187953065  raw/images/187953065/1.jpg  raw/images/187953065/3.jpg   \n",
       "\n",
       "                 Accessoire_imagePath             Shoes_imagePath  \\\n",
       "120161271                        None                        None   \n",
       "143656996  raw/images/143656996/5.jpg                        None   \n",
       "216470135                        None  raw/images/216470135/3.jpg   \n",
       "216220312  raw/images/216220312/4.jpg  raw/images/216220312/3.jpg   \n",
       "192203629  raw/images/192203629/5.jpg                        None   \n",
       "...                               ...                         ...   \n",
       "201717504  raw/images/201717504/4.jpg  raw/images/201717504/3.jpg   \n",
       "216589548  raw/images/216589548/5.jpg                        None   \n",
       "216860218  raw/images/216860218/5.jpg                        None   \n",
       "211099953                        None  raw/images/211099953/3.jpg   \n",
       "187953065  raw/images/187953065/5.jpg  raw/images/187953065/4.jpg   \n",
       "\n",
       "                  Outerwear_imagePath  valid_outfit  \n",
       "120161271                        None             0  \n",
       "143656996                        None             0  \n",
       "216470135                        None             1  \n",
       "216220312                        None             1  \n",
       "192203629  raw/images/192203629/1.jpg             0  \n",
       "...                               ...           ...  \n",
       "201717504                        None             1  \n",
       "216589548  raw/images/216589548/2.jpg             1  \n",
       "216860218  raw/images/216860218/2.jpg             1  \n",
       "211099953                        None             1  \n",
       "187953065  raw/images/187953065/2.jpg             0  \n",
       "\n",
       "[6723 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    dataset_path\n",
    ")\n",
    "#df = df.iloc[:100].copy()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Innerwear_imagePath</th>\n",
       "      <th>Bottomwear_imagePath</th>\n",
       "      <th>Accessoire_imagePath</th>\n",
       "      <th>Shoes_imagePath</th>\n",
       "      <th>Outerwear_imagePath</th>\n",
       "      <th>valid_outfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140890817</th>\n",
       "      <td>raw/images/140890817/1.jpg</td>\n",
       "      <td>raw/images/140890817/2.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203955931</th>\n",
       "      <td>raw/images/203955931/1.jpg</td>\n",
       "      <td>raw/images/203955931/2.jpg</td>\n",
       "      <td>raw/images/203955931/3.jpg</td>\n",
       "      <td>raw/images/203955931/7.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216947310</th>\n",
       "      <td>raw/images/216947310/1.jpg</td>\n",
       "      <td>raw/images/216947310/3.jpg</td>\n",
       "      <td>raw/images/216947310/5.jpg</td>\n",
       "      <td>raw/images/216947310/4.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210526001</th>\n",
       "      <td>raw/images/210526001/1.jpg</td>\n",
       "      <td>raw/images/210526001/2.jpg</td>\n",
       "      <td>raw/images/210526001/4.jpg</td>\n",
       "      <td>raw/images/210526001/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215262893</th>\n",
       "      <td>raw/images/215262893/1.jpg</td>\n",
       "      <td>raw/images/215262893/2.jpg</td>\n",
       "      <td>raw/images/215262893/4.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146338069</th>\n",
       "      <td>raw/images/146338069/1.jpg</td>\n",
       "      <td>raw/images/146338069/2.jpg</td>\n",
       "      <td>raw/images/146338069/4.jpg</td>\n",
       "      <td>raw/images/146338069/3.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216252775</th>\n",
       "      <td>raw/images/216252775/1.jpg</td>\n",
       "      <td>raw/images/216252775/3.jpg</td>\n",
       "      <td>raw/images/216252775/5.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>raw/images/216252775/2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213715463</th>\n",
       "      <td>raw/images/213715463/1.jpg</td>\n",
       "      <td>raw/images/213715463/2.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170950241</th>\n",
       "      <td>raw/images/170950241/1.jpg</td>\n",
       "      <td>raw/images/170950241/3.jpg</td>\n",
       "      <td>raw/images/170950241/5.jpg</td>\n",
       "      <td>raw/images/170950241/4.jpg</td>\n",
       "      <td>raw/images/170950241/2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117105911</th>\n",
       "      <td>raw/images/117105911/1.jpg</td>\n",
       "      <td>raw/images/117105911/2.jpg</td>\n",
       "      <td>raw/images/117105911/4.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4033 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Innerwear_imagePath        Bottomwear_imagePath  \\\n",
       "140890817  raw/images/140890817/1.jpg  raw/images/140890817/2.jpg   \n",
       "203955931  raw/images/203955931/1.jpg  raw/images/203955931/2.jpg   \n",
       "216947310  raw/images/216947310/1.jpg  raw/images/216947310/3.jpg   \n",
       "210526001  raw/images/210526001/1.jpg  raw/images/210526001/2.jpg   \n",
       "215262893  raw/images/215262893/1.jpg  raw/images/215262893/2.jpg   \n",
       "...                               ...                         ...   \n",
       "146338069  raw/images/146338069/1.jpg  raw/images/146338069/2.jpg   \n",
       "216252775  raw/images/216252775/1.jpg  raw/images/216252775/3.jpg   \n",
       "213715463  raw/images/213715463/1.jpg  raw/images/213715463/2.jpg   \n",
       "170950241  raw/images/170950241/1.jpg  raw/images/170950241/3.jpg   \n",
       "117105911  raw/images/117105911/1.jpg  raw/images/117105911/2.jpg   \n",
       "\n",
       "                 Accessoire_imagePath             Shoes_imagePath  \\\n",
       "140890817                        None                        None   \n",
       "203955931  raw/images/203955931/3.jpg  raw/images/203955931/7.jpg   \n",
       "216947310  raw/images/216947310/5.jpg  raw/images/216947310/4.jpg   \n",
       "210526001  raw/images/210526001/4.jpg  raw/images/210526001/3.jpg   \n",
       "215262893  raw/images/215262893/4.jpg                        None   \n",
       "...                               ...                         ...   \n",
       "146338069  raw/images/146338069/4.jpg  raw/images/146338069/3.jpg   \n",
       "216252775  raw/images/216252775/5.jpg                        None   \n",
       "213715463                        None                        None   \n",
       "170950241  raw/images/170950241/5.jpg  raw/images/170950241/4.jpg   \n",
       "117105911  raw/images/117105911/4.jpg                        None   \n",
       "\n",
       "                  Outerwear_imagePath  valid_outfit  \n",
       "140890817                        None             0  \n",
       "203955931                        None             0  \n",
       "216947310                        None             0  \n",
       "210526001                        None             0  \n",
       "215262893                        None             1  \n",
       "...                               ...           ...  \n",
       "146338069                        None             0  \n",
       "216252775  raw/images/216252775/2.jpg             1  \n",
       "213715463                        None             1  \n",
       "170950241  raw/images/170950241/2.jpg             1  \n",
       "117105911                        None             0  \n",
       "\n",
       "[4033 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.20, random_state=42, stratify=df['valid_outfit'])\n",
    "train, validation = train_test_split(train, test_size=0.25, random_state=42, stratify=train['valid_outfit'])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training dataset: 100%|██████████| 4033/4033 [05:44<00:00, 11.69it/s]\n",
      "Loading validation dataset: 100%|██████████| 1345/1345 [01:46<00:00, 12.64it/s]\n",
      "Loading test dataset: 100%|██████████| 1345/1345 [01:41<00:00, 13.26it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CROP_SIZE = 256\n",
    "BATCH_SIZE = model_config['batch_size']\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "image_datasets = {\n",
    "    'train': CustomDataset(train, data_transforms['train'], 'training'),\n",
    "    'val': CustomDataset(validation, data_transforms['val'], 'validation'),\n",
    "    'test': CustomDataset(test, data_transforms['test'], 'test')\n",
    "}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "def get_image_for_matplot_lib(img):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    #fig = plt.figure(figsize=(cols * 3, rows * 3))\n",
    "    img = img.cpu().numpy().transpose((1, 2, 0))\n",
    "    img = std * img + mean\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "\n",
    "def show_batch(imgs, classification):\n",
    "    number_of_clothing_items = len(imgs)\n",
    "\n",
    "    number_of_batch_rows = len(imgs[0])\n",
    "\n",
    "    for batch_item_index in range(number_of_batch_rows):\n",
    "        f, axarr = plt.subplots(1, number_of_clothing_items + 1, figsize=(15, 2))\n",
    "        f.patch.set_facecolor('black')\n",
    "\n",
    "        clothing_item_accessoire = get_image_for_matplot_lib(imgs[0][batch_item_index][0])\n",
    "        clothing_item_inner_wear = get_image_for_matplot_lib(imgs[1][batch_item_index][0])\n",
    "        clothing_item_bottom_wear = get_image_for_matplot_lib(imgs[2][batch_item_index][0])\n",
    "        clothing_item_shoes = get_image_for_matplot_lib(imgs[3][batch_item_index][0])\n",
    "        clothing_item_over_wear = get_image_for_matplot_lib(imgs[4][batch_item_index][0])\n",
    "\n",
    "        clothing_items = [clothing_item_accessoire, clothing_item_over_wear, clothing_item_inner_wear,\n",
    "                          clothing_item_bottom_wear, clothing_item_shoes]\n",
    "        for cloting_item_axis_index in range(len(clothing_items)):\n",
    "            ax = axarr[cloting_item_axis_index]\n",
    "            ax.imshow(clothing_items[cloting_item_axis_index])\n",
    "            ax.axis('off')\n",
    "\n",
    "        ax = axarr[cloting_item_axis_index + 1]\n",
    "        ax.set_xlim([0, 0.5])\n",
    "        ax.set_ylim([0, 0.5])\n",
    "\n",
    "        is_a_good_outfit = classification[batch_item_index] == 1\n",
    "\n",
    "        label_font_size = 20\n",
    "        if is_a_good_outfit:\n",
    "            ax.text(0.5, 0.5, 'good', horizontalalignment='center', transform=ax.transAxes, weight='bold',\n",
    "                    color='green', fontsize=label_font_size)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'bad', horizontalalignment='center', transform=ax.transAxes, weight='bold', color='red',\n",
    "                    fontsize=label_font_size)\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "        f.tight_layout()\n",
    "\n",
    "\n",
    "inputs, classification = next(iter(dataloaders['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_batch(inputs, classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, feature_model, loss_fn, optimizer):\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    feature_model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    feature_model.counter = 0\n",
    "\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Training', total=num_batches):\n",
    "        # Compute prediction and loss\n",
    "        pred = feature_model(X)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Compute L1 loss component\n",
    "        if model_config['regularisation'] == 'l1':\n",
    "            # lasso regularization\n",
    "            l1_weight = model_config['regularisation_weight']\n",
    "            l1_parameters = []\n",
    "            for parameter in feature_model.trainable_model.parameters():\n",
    "                l1_parameters.append(parameter.view(-1))\n",
    "            l1 = l1_weight * feature_model.compute_l1_loss(torch.cat(l1_parameters))\n",
    "\n",
    "            # Add L1 loss component\n",
    "            loss += l1\n",
    "        elif model_config['regularisation'] == 'l2':\n",
    "            #todo: implement\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f'configuration value for key regularisation was {model_config[\"regularisation\"]} which is not a valid configuraiton value')\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / num_batches\n",
    "    epoch_acc = running_corrects / size\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def val_loop(dataloader, feature_model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    feature_model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, val_acc = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader, desc='Validation', total=num_batches):\n",
    "            pred = feature_model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            val_acc += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_acc /= size\n",
    "\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutfitClassifier(\n",
       "  (trainable_model): Sequential(\n",
       "    (0): Linear(in_features=1920, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfit_classifier = OutfitClassifier().to(device)\n",
    "\n",
    "outfit_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdata_scientist_24\u001B[0m (\u001B[33mrz_datascience\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO 2/basic-models/wandb/run-20240518_172149-eezhm1cc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29/runs/eezhm1cc' target=\"_blank\">brisk-cosmos-154</a></strong> to <a href='https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29' target=\"_blank\">https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29/runs/eezhm1cc' target=\"_blank\">https://wandb.ai/rz_datascience/ReWear%20-%20Outfit%20Recommender%20%28DSPRO2%29/runs/eezhm1cc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 131329\n",
      "total trainable params trainable model: 131329\n"
     ]
    }
   ],
   "source": [
    "outfit_classifier = OutfitClassifier()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ReWear - Outfit Recommender (DSPRO2)\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config=model_config\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    outfit_classifier.parameters(),\n",
    "    lr=model_config['init_lr']\n",
    ")\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     linear_classifier.parameters(),\n",
    "#     lr=model_config['init_lr'],\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0, # we do not apply weight decay\n",
    "# )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, model_config['epochs'], eta_min=0)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in outfit_classifier.parameters() if p.requires_grad)\n",
    "pytorch_total_params_trainable = sum(\n",
    "    p.numel() for p in outfit_classifier.trainable_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'total trainable params: {pytorch_total_params}')\n",
    "print(f'total trainable params trainable model: {pytorch_total_params_trainable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/127 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 127/127 [02:54<00:00,  1.38s/it]\n",
      "Validation: 100%|██████████| 43/43 [00:52<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy = 0.5479553903345725, best_loss = 0.690824102523715\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 127/127 [02:54<00:00,  1.37s/it]\n",
      "Validation:   9%|▉         | 4/43 [00:04<00:48,  1.24s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(config, data_dir=None):\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ReWear - Outfit Recommender (DSPRO2) Dino v2 based\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    VAL_FREQ = 1\n",
    "    best_acc = 0.0\n",
    "    best_acc_loss = np.inf\n",
    "    train_data = []\n",
    "    for t in range(model_config['epochs']):\n",
    "        print(f'Epoch {t + 1}\\n-------------------------------')\n",
    "\n",
    "        train_acc, train_loss = train_loop(dataloaders['train'], outfit_classifier, loss_fn, optimizer)\n",
    "        train_data.append({\n",
    "            'phase': 'train',\n",
    "            'epoch': t,\n",
    "            'lr': optimizer.param_groups[0][\"lr\"],\n",
    "            'accuracy': train_acc,\n",
    "            'loss': train_loss,\n",
    "        })\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_acc, val_loss = val_loop(dataloaders['val'], outfit_classifier, loss_fn)\n",
    "        train_data.append({\n",
    "            'phase': 'val',\n",
    "            'epoch': t,\n",
    "            'lr': optimizer.param_groups[0][\"lr\"],\n",
    "            'accuracy': val_acc,\n",
    "            'loss': val_loss\n",
    "        })\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'epoch': t,\n",
    "                'lr': optimizer.param_groups[0][\"lr\"],\n",
    "                'training_accuracy': train_acc,\n",
    "                'training_loss': train_loss,\n",
    "                'validation_accuracy': val_acc,\n",
    "                'validation_loss': val_loss\n",
    "            }\n",
    "        )\n",
    "        #print(f'Validation:\\n    val_acc = {val_acc}, val_loss = {val_loss}')\n",
    "        if (val_acc == best_acc and val_loss < best_acc_loss) or (val_acc > best_acc):\n",
    "            best_acc, best_acc_loss = val_acc, val_loss\n",
    "            save_dict = {\n",
    "                'epoch': t + 1,\n",
    "                'state_dict': outfit_classifier.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'best_loss': best_acc_loss\n",
    "            }\n",
    "            torch.save(save_dict, os.path.join(wandb.run.dir, 'dino_classifier_ckpt.pth'))\n",
    "\n",
    "        print('\\n')\n",
    "    print('Training completed.')\n",
    "    wandb.save('dino_classifier_ckpt.pth')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "config = {\n",
    "    # \"l1\": tune.choice([2 ** i for i in range(9)]),\n",
    "    # \"l2\": tune.choice([2 ** i for i in range(9)]),\n",
    "    # \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    # \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    'init_lr': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 25,\n",
    "    'empty_image_representation': 'zero_matrix',  #  zero_matrix, torch_empty\n",
    "    'dino_architecture': 'small',\n",
    "    'training_dataset': 'polyvore_63eb50dc58d97415384467bef7b3c9e1bd6c96e06ad19571b6bc15e9dd5af262.parquet',\n",
    "    'testing_dataset': '',\n",
    "    'model_forward_version': 'fast',  # slow, fast\n",
    "    'hidden_layer_neuron_count': 64,\n",
    "    'dropout_probability': 0.1,\n",
    "}\n",
    "\n",
    "tune.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
