{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import ssl\n",
    "import pandas as pd\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision import models as torchvision_models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/romanzberg/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /Users/romanzberg/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /Users/romanzberg/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /Users/romanzberg/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /Users/romanzberg/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "accessoire_dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "inner_wear_dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "outer_wear_dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "bottom_wear_dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "shoes_dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "384"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessoire_dinov2_vitb14.embed_dim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# freeze weights of models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "for param in accessoire_dinov2_vitb14.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in inner_wear_dinov2_vitb14.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in outer_wear_dinov2_vitb14.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in bottom_wear_dinov2_vitb14.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in shoes_dinov2_vitb14.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Get ImageNet labels\n",
    "imagenet_class_url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "imagenet_classes = json.loads(urllib.request.urlopen(imagenet_class_url).read())\n",
    "\n",
    "\n",
    "# # Set a device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#https://www.modelbit.com/blog/deploying-dinov2-for-image-classification-with-modelbit#:~:text=To%20do%20this%2C%20simply%20use,machine%20to%20your%20Colab%20directory.&text=Next%2C%20you'll%20want%20to,ImageNet%20preprocessing%20on%20the%20image.&text=Now%2C%20we%20can%20pass%20the,a%20class%20ID%20and%20label.\n",
    "def dinov2_classifier(img_url):\n",
    "    response = requests.get(img_url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Preprocess the image\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "\n",
    "    # Move the image to the GPU if available\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Extract the features\n",
    "    with torch.no_grad():\n",
    "        features = accessoire_dinov2_vitb14(image.unsqueeze(0))\n",
    "\n",
    "    # Print the features\n",
    "    return {'index': features.argmax(-1).item(),\n",
    "            'label': imagenet_classes[features.argmax(-1).item()]\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "{'index': 229, 'label': 'Old English Sheepdog'}"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinov2_classifier(\n",
    "    \"https://www.baunat.com/de/15260067_CG-GY-PG-0050R_1_With-chain_722x722/2-00-karat-solitar-anhanger-aus-gelbgold-mit-rundem-diamant-und-4-krappen.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing Custom Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'use_n_blocks': 1,\n",
    "    'init_lr': 0.0002,\n",
    "    'epochs': 20,\n",
    "}\n",
    "\n",
    "\n",
    "# class for DINOv2\n",
    "class ModelWithIntermediateLayers(nn.Module):\n",
    "    def __init__(self, feature_model, n_last_blocks):\n",
    "        super().__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_model.eval()\n",
    "        self.n_last_blocks = n_last_blocks\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.inference_mode():\n",
    "            features = self.feature_model.get_intermediate_layers(\n",
    "                image, self.n_last_blocks, return_class_token=True\n",
    "            )\n",
    "        return features\n",
    "\n",
    "\n",
    "def create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n",
    "    intermediate_output = x_tokens_list[-use_n_blocks:]\n",
    "    output = torch.cat([class_token for _, class_token in intermediate_output], dim=-1)\n",
    "    if use_avgpool:\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                output,\n",
    "                torch.mean(intermediate_output[-1][0], dim=1),  # patch tokens\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "    return output.float()\n",
    "\n",
    "\n",
    "class OutfitClassifier(nn.Module):\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self):\n",
    "        return self._embed_dim\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OutfitClassifier, self).__init__()\n",
    "        self._accessoire_model = ModelWithIntermediateLayers(accessoire_dinov2_vitb14, model_config['use_n_blocks'])\n",
    "        self._inner_wear_model = ModelWithIntermediateLayers(inner_wear_dinov2_vitb14, model_config['use_n_blocks'])\n",
    "        self._outer_wear_model = ModelWithIntermediateLayers(outer_wear_dinov2_vitb14, model_config['use_n_blocks'])\n",
    "        self._bottom_wear_model = ModelWithIntermediateLayers(bottom_wear_dinov2_vitb14, model_config['use_n_blocks'])\n",
    "        self._shoes_model = ModelWithIntermediateLayers(shoes_dinov2_vitb14, model_config['use_n_blocks'])\n",
    "        self._embed_dim = accessoire_dinov2_vitb14.embed_dim * (model_config['use_n_blocks']) * 5\n",
    "\n",
    "        self._accessoire_model.to(device)\n",
    "        self._inner_wear_model.to(device)\n",
    "        self._outer_wear_model.to(device)\n",
    "        self._bottom_wear_model.to(device)\n",
    "        self._shoes_model.to(device)\n",
    "\n",
    "        self.trainable_model = nn.Sequential(\n",
    "            nn.Linear(self._embed_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.trainable_model.eval()\n",
    "        self.trainable_model.to(device)\n",
    "        self.counter = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        number_of_features = len(X)\n",
    "        # print()\n",
    "        # print(X[0].shape)\n",
    "        # print(X[1].shape)\n",
    "        # print(X[2].shape)\n",
    "        # print(X[3].shape)\n",
    "        # print(X[4].shape)\n",
    "        # print(X)\n",
    "        feature_list = []\n",
    "        # for x in X:\n",
    "        # x = X[0]\n",
    "\n",
    "        number_of_rows = X[0].shape[0]\n",
    "        # print(f'number of rows: {number_of_rows}')\n",
    "        with torch.no_grad():\n",
    "            for index in range(number_of_rows):\n",
    "                extracted_features = torch.cat(\n",
    "                    (\n",
    "                        create_linear_input(self._accessoire_model.forward(X[0][index]), model_config['use_n_blocks'],\n",
    "                                            False),\n",
    "                        create_linear_input(self._inner_wear_model.forward(X[1][index]), model_config['use_n_blocks'],\n",
    "                                            False),\n",
    "                        create_linear_input(self._outer_wear_model.forward(X[2][index]), model_config['use_n_blocks'],\n",
    "                                            False),\n",
    "                        create_linear_input(self._bottom_wear_model.forward(X[3][index]), model_config['use_n_blocks'],\n",
    "                                            False),\n",
    "                        create_linear_input(self._shoes_model.forward(X[4][index]), model_config['use_n_blocks'], False)\n",
    "                    ),\n",
    "                    dim=1\n",
    "                )\n",
    "                extracted_features.requires_grad_()\n",
    "\n",
    "                feature_list.append(extracted_features)\n",
    "\n",
    "        predictions = []\n",
    "        # print(f'length of featurs: {len(feature_list)}')\n",
    "        for features in feature_list:\n",
    "            prediction = self.trainable_model.forward(features)\n",
    "            predictions.append(torch.squeeze(prediction).nan_to_num(0))\n",
    "\n",
    "        self.counter += 1\n",
    "        print(f'finished {self.counter * 8}')\n",
    "\n",
    "        return torch.squeeze(torch.tensor(predictions)).to(torch.float)\n",
    "\n",
    "        # return self.trainable_model.forward(extracted_features)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.use_n_blocks = config['use_n_blocks']\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = create_linear_input(input, self.use_n_blocks, False)\n",
    "        return self.model(output)\n",
    "\n",
    "\n",
    "outfit_classifier = OutfitClassifier()\n",
    "classifier = Classifier(outfit_classifier.embed_dim, model_config)\n",
    "classifier = classifier.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# Class for test dataset\n",
    "def get_image(img_path, tfms: v2.Compose):\n",
    "    if img_path is None:\n",
    "        return torch.empty(3, 224, 224)\n",
    "    else:\n",
    "        return tfms(read_image(f'../datasets/{img_path}'))\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_frame, tfms: v2.Compose):\n",
    "        self.df = data_frame\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        outfit = self.df.iloc[index]\n",
    "        img_accessoire = get_image(outfit['Accessoire_imagePath'], self.tfms)\n",
    "        img_innerwear = get_image(outfit['Innerwear_imagePath'], self.tfms)\n",
    "        img_bottomwear = get_image(outfit['Bottomwear_imagePath'], self.tfms)\n",
    "        img_shoe = get_image(outfit['Shoes_imagePath'], self.tfms)\n",
    "        img_outerwear = get_image(outfit['Outerwear_imagePath'], self.tfms)\n",
    "\n",
    "        # print(f'accessoire shape: {img_accessoire.shape}; path={outfit[\"Accessoire_imagePath\"]}')\n",
    "        # print(f'innerwear shape: {img_innerwear.shape}; path={outfit[\"Innerwear_imagePath\"]}')\n",
    "\n",
    "        target_variable = torch.tensor(outfit['valid_outfit']).to(torch.float)\n",
    "        target_variable.requires_grad_()\n",
    "\n",
    "        return (\n",
    "                   img_accessoire.unsqueeze(0),\n",
    "                   img_innerwear.unsqueeze(0),\n",
    "                   img_bottomwear.unsqueeze(0),\n",
    "                   img_shoe.unsqueeze(0),\n",
    "                   img_outerwear.unsqueeze(0)\n",
    "               ), target_variable\n",
    "\n",
    "    def __len__(self):\n",
    "        return df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "                  Innerwear_imagePath        Bottomwear_imagePath  \\\n120161271  raw/images/120161271/1.jpg  raw/images/120161271/2.jpg   \n143656996  raw/images/143656996/1.jpg  raw/images/143656996/3.jpg   \n216470135  raw/images/216470135/1.jpg  raw/images/216470135/2.jpg   \n216220312  raw/images/216220312/1.jpg  raw/images/216220312/2.jpg   \n192203629  raw/images/192203629/2.jpg  raw/images/192203629/3.jpg   \n...                               ...                         ...   \n201717504  raw/images/201717504/1.jpg  raw/images/201717504/2.jpg   \n216589548  raw/images/216589548/1.jpg  raw/images/216589548/3.jpg   \n216860218  raw/images/216860218/1.jpg  raw/images/216860218/3.jpg   \n211099953  raw/images/211099953/1.jpg  raw/images/211099953/2.jpg   \n187953065  raw/images/187953065/1.jpg  raw/images/187953065/3.jpg   \n\n                 Accessoire_imagePath             Shoes_imagePath  \\\n120161271                        None                        None   \n143656996  raw/images/143656996/5.jpg                        None   \n216470135                        None  raw/images/216470135/3.jpg   \n216220312  raw/images/216220312/4.jpg  raw/images/216220312/3.jpg   \n192203629  raw/images/192203629/5.jpg                        None   \n...                               ...                         ...   \n201717504  raw/images/201717504/4.jpg  raw/images/201717504/3.jpg   \n216589548  raw/images/216589548/5.jpg                        None   \n216860218  raw/images/216860218/5.jpg                        None   \n211099953                        None  raw/images/211099953/3.jpg   \n187953065  raw/images/187953065/5.jpg  raw/images/187953065/4.jpg   \n\n                  Outerwear_imagePath  valid_outfit  \n120161271                        None             0  \n143656996                        None             0  \n216470135                        None             1  \n216220312                        None             1  \n192203629  raw/images/192203629/1.jpg             0  \n...                               ...           ...  \n201717504                        None             1  \n216589548  raw/images/216589548/2.jpg             1  \n216860218  raw/images/216860218/2.jpg             1  \n211099953                        None             1  \n187953065  raw/images/187953065/2.jpg             0  \n\n[6723 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Innerwear_imagePath</th>\n      <th>Bottomwear_imagePath</th>\n      <th>Accessoire_imagePath</th>\n      <th>Shoes_imagePath</th>\n      <th>Outerwear_imagePath</th>\n      <th>valid_outfit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>120161271</th>\n      <td>raw/images/120161271/1.jpg</td>\n      <td>raw/images/120161271/2.jpg</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>143656996</th>\n      <td>raw/images/143656996/1.jpg</td>\n      <td>raw/images/143656996/3.jpg</td>\n      <td>raw/images/143656996/5.jpg</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>216470135</th>\n      <td>raw/images/216470135/1.jpg</td>\n      <td>raw/images/216470135/2.jpg</td>\n      <td>None</td>\n      <td>raw/images/216470135/3.jpg</td>\n      <td>None</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216220312</th>\n      <td>raw/images/216220312/1.jpg</td>\n      <td>raw/images/216220312/2.jpg</td>\n      <td>raw/images/216220312/4.jpg</td>\n      <td>raw/images/216220312/3.jpg</td>\n      <td>None</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>192203629</th>\n      <td>raw/images/192203629/2.jpg</td>\n      <td>raw/images/192203629/3.jpg</td>\n      <td>raw/images/192203629/5.jpg</td>\n      <td>None</td>\n      <td>raw/images/192203629/1.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>201717504</th>\n      <td>raw/images/201717504/1.jpg</td>\n      <td>raw/images/201717504/2.jpg</td>\n      <td>raw/images/201717504/4.jpg</td>\n      <td>raw/images/201717504/3.jpg</td>\n      <td>None</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216589548</th>\n      <td>raw/images/216589548/1.jpg</td>\n      <td>raw/images/216589548/3.jpg</td>\n      <td>raw/images/216589548/5.jpg</td>\n      <td>None</td>\n      <td>raw/images/216589548/2.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216860218</th>\n      <td>raw/images/216860218/1.jpg</td>\n      <td>raw/images/216860218/3.jpg</td>\n      <td>raw/images/216860218/5.jpg</td>\n      <td>None</td>\n      <td>raw/images/216860218/2.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211099953</th>\n      <td>raw/images/211099953/1.jpg</td>\n      <td>raw/images/211099953/2.jpg</td>\n      <td>None</td>\n      <td>raw/images/211099953/3.jpg</td>\n      <td>None</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>187953065</th>\n      <td>raw/images/187953065/1.jpg</td>\n      <td>raw/images/187953065/3.jpg</td>\n      <td>raw/images/187953065/5.jpg</td>\n      <td>raw/images/187953065/4.jpg</td>\n      <td>raw/images/187953065/2.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6723 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    '../datasets/imageBasedModel/polyvore/polyvore_63eb50dc58d97415384467bef7b3c9e1bd6c96e06ad19571b6bc15e9dd5af262.parquet'\n",
    ")\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "CROP_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "data_transforms = v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    v2.AutoAugment(v2.AutoAugmentPolicy.IMAGENET),\n",
    "    v2.ConvertImageDtype(torch.float32),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "image_datasets = {}\n",
    "image_datasets['train'], image_datasets['val'], image_datasets['test'] = torch.utils.data.random_split(\n",
    "    CustomDataset(df, data_transforms), [0.8, 0.10, 0.10], generator=generator1)\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "               for x in ['train', 'val']}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, feature_model, loss_fn, optimizer):\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    feature_model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = [x.to(device) for x in X]\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = feature_model(X)\n",
    "        # print('pred:', pred)\n",
    "        # print('y:', y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / num_batches\n",
    "    epoch_acc = running_corrects / size\n",
    "    return epoch_acc, epoch_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def val_loop(dataloader, feature_model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    feature_model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, val_acc = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X\n",
    "            y = y\n",
    "            pred = feature_model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            val_acc += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_acc /= size\n",
    "    return val_acc, val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 30753\n"
     ]
    }
   ],
   "source": [
    "outfit_classifier = OutfitClassifier()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    outfit_classifier.parameters(),\n",
    "    lr=model_config['init_lr'],\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     linear_classifier.parameters(),\n",
    "#     lr=model_config['init_lr'],\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0, # we do not apply weight decay\n",
    "# )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, model_config['epochs'], eta_min=0)\n",
    "pytorch_total_params = sum(p.numel() for p in outfit_classifier.parameters() if p.requires_grad)\n",
    "print(f'total trainable params: {pytorch_total_params}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "finished 8\n",
      "finished 16\n",
      "finished 24\n",
      "finished 32\n",
      "finished 40\n",
      "finished 48\n",
      "finished 56\n",
      "finished 64\n",
      "finished 72\n",
      "finished 80\n",
      "finished 88\n",
      "finished 96\n",
      "finished 104\n",
      "finished 112\n",
      "finished 120\n",
      "finished 128\n",
      "finished 136\n",
      "finished 144\n",
      "finished 152\n",
      "finished 160\n",
      "finished 168\n",
      "finished 176\n",
      "finished 184\n",
      "finished 192\n",
      "finished 200\n",
      "finished 208\n",
      "finished 216\n",
      "finished 224\n",
      "finished 232\n",
      "finished 240\n",
      "finished 248\n",
      "finished 256\n",
      "finished 264\n",
      "finished 272\n",
      "finished 280\n",
      "finished 288\n",
      "finished 296\n",
      "finished 304\n",
      "finished 312\n",
      "finished 320\n",
      "finished 328\n",
      "finished 336\n",
      "finished 344\n",
      "finished 352\n",
      "finished 360\n",
      "finished 368\n",
      "finished 376\n",
      "finished 384\n",
      "finished 392\n",
      "finished 400\n",
      "finished 408\n",
      "finished 416\n",
      "finished 424\n",
      "finished 432\n",
      "finished 440\n",
      "finished 448\n",
      "finished 456\n",
      "finished 464\n",
      "finished 472\n",
      "finished 480\n",
      "finished 488\n",
      "finished 496\n",
      "finished 504\n",
      "finished 512\n",
      "finished 520\n",
      "finished 528\n",
      "finished 536\n",
      "finished 544\n",
      "finished 552\n",
      "finished 560\n",
      "finished 568\n",
      "finished 576\n",
      "finished 584\n",
      "finished 592\n",
      "finished 600\n",
      "finished 608\n",
      "finished 616\n",
      "finished 624\n",
      "finished 632\n",
      "finished 640\n",
      "finished 648\n",
      "finished 656\n",
      "finished 664\n",
      "finished 672\n",
      "finished 680\n",
      "finished 688\n",
      "finished 696\n",
      "finished 704\n",
      "finished 712\n",
      "finished 720\n",
      "finished 728\n",
      "finished 736\n",
      "finished 744\n",
      "finished 752\n",
      "finished 760\n",
      "finished 768\n",
      "finished 776\n",
      "finished 784\n",
      "finished 792\n",
      "finished 800\n",
      "finished 808\n",
      "finished 816\n",
      "finished 824\n",
      "finished 832\n",
      "finished 840\n",
      "finished 848\n",
      "finished 856\n",
      "finished 864\n",
      "finished 872\n",
      "finished 880\n",
      "finished 888\n",
      "finished 896\n",
      "finished 904\n",
      "finished 912\n",
      "finished 920\n",
      "finished 928\n",
      "finished 936\n",
      "finished 944\n",
      "finished 952\n",
      "finished 960\n",
      "finished 968\n",
      "finished 976\n",
      "finished 984\n",
      "finished 992\n",
      "finished 1000\n",
      "finished 1008\n",
      "finished 1016\n",
      "finished 1024\n",
      "finished 1032\n",
      "finished 1040\n",
      "finished 1048\n",
      "finished 1056\n",
      "finished 1064\n",
      "finished 1072\n",
      "finished 1080\n",
      "finished 1088\n",
      "finished 1096\n",
      "finished 1104\n",
      "finished 1112\n",
      "finished 1120\n",
      "finished 1128\n",
      "finished 1136\n",
      "finished 1144\n",
      "finished 1152\n",
      "finished 1160\n",
      "finished 1168\n",
      "finished 1176\n",
      "finished 1184\n",
      "finished 1192\n",
      "finished 1200\n",
      "finished 1208\n",
      "finished 1216\n",
      "finished 1224\n",
      "finished 1232\n",
      "finished 1240\n",
      "finished 1248\n",
      "finished 1256\n",
      "finished 1264\n",
      "finished 1272\n",
      "finished 1280\n",
      "finished 1288\n",
      "finished 1296\n",
      "finished 1304\n",
      "finished 1312\n",
      "finished 1320\n",
      "finished 1328\n",
      "finished 1336\n",
      "finished 1344\n",
      "finished 1352\n",
      "finished 1360\n",
      "finished 1368\n",
      "finished 1376\n",
      "finished 1384\n",
      "finished 1392\n",
      "finished 1400\n",
      "finished 1408\n",
      "finished 1416\n",
      "finished 1424\n",
      "finished 1432\n",
      "finished 1440\n",
      "finished 1448\n",
      "finished 1456\n",
      "finished 1464\n",
      "finished 1472\n",
      "finished 1480\n",
      "finished 1488\n",
      "finished 1496\n",
      "finished 1504\n",
      "finished 1512\n",
      "finished 1520\n",
      "finished 1528\n",
      "finished 1536\n",
      "finished 1544\n",
      "finished 1552\n",
      "finished 1560\n",
      "finished 1568\n",
      "finished 1576\n",
      "finished 1584\n",
      "finished 1592\n",
      "finished 1600\n",
      "finished 1608\n",
      "finished 1616\n",
      "finished 1624\n",
      "finished 1632\n",
      "finished 1640\n",
      "finished 1648\n",
      "finished 1656\n",
      "finished 1664\n",
      "finished 1672\n",
      "finished 1680\n",
      "finished 1688\n",
      "finished 1696\n",
      "finished 1704\n",
      "finished 1712\n",
      "finished 1720\n",
      "finished 1728\n",
      "finished 1736\n",
      "finished 1744\n",
      "finished 1752\n",
      "finished 1760\n",
      "finished 1768\n",
      "finished 1776\n",
      "finished 1784\n",
      "finished 1792\n",
      "finished 1800\n",
      "finished 1808\n",
      "finished 1816\n",
      "finished 1824\n",
      "finished 1832\n",
      "finished 1840\n",
      "finished 1848\n",
      "finished 1856\n",
      "finished 1864\n",
      "finished 1872\n",
      "finished 1880\n",
      "finished 1888\n",
      "finished 1896\n",
      "finished 1904\n",
      "finished 1912\n",
      "finished 1920\n",
      "finished 1928\n",
      "finished 1936\n",
      "finished 1944\n",
      "finished 1952\n",
      "finished 1960\n",
      "finished 1968\n",
      "finished 1976\n",
      "finished 1984\n",
      "finished 1992\n",
      "finished 2000\n",
      "finished 2008\n",
      "finished 2016\n",
      "finished 2024\n",
      "finished 2032\n",
      "finished 2040\n",
      "finished 2048\n",
      "finished 2056\n",
      "finished 2064\n",
      "finished 2072\n",
      "finished 2080\n",
      "finished 2088\n",
      "finished 2096\n",
      "finished 2104\n",
      "finished 2112\n",
      "finished 2120\n",
      "finished 2128\n",
      "finished 2136\n",
      "finished 2144\n",
      "finished 2152\n",
      "finished 2160\n",
      "finished 2168\n",
      "finished 2176\n",
      "finished 2184\n",
      "finished 2192\n",
      "finished 2200\n",
      "finished 2208\n",
      "finished 2216\n",
      "finished 2224\n",
      "finished 2232\n",
      "finished 2240\n",
      "finished 2248\n",
      "finished 2256\n",
      "finished 2264\n",
      "finished 2272\n",
      "finished 2280\n",
      "finished 2288\n",
      "finished 2296\n",
      "finished 2304\n",
      "finished 2312\n",
      "finished 2320\n",
      "finished 2328\n",
      "finished 2336\n",
      "finished 2344\n",
      "finished 2352\n",
      "finished 2360\n",
      "finished 2368\n",
      "finished 2376\n",
      "finished 2384\n",
      "finished 2392\n",
      "finished 2400\n",
      "finished 2408\n",
      "finished 2416\n",
      "finished 2424\n",
      "finished 2432\n",
      "finished 2440\n",
      "finished 2448\n",
      "finished 2456\n",
      "finished 2464\n",
      "finished 2472\n",
      "finished 2480\n",
      "finished 2488\n",
      "finished 2496\n",
      "finished 2504\n",
      "finished 2512\n",
      "finished 2520\n",
      "finished 2528\n",
      "finished 2536\n",
      "finished 2544\n",
      "finished 2552\n",
      "finished 2560\n",
      "finished 2568\n",
      "finished 2576\n",
      "finished 2584\n",
      "finished 2592\n",
      "finished 2600\n",
      "finished 2608\n",
      "finished 2616\n",
      "finished 2624\n",
      "finished 2632\n",
      "finished 2640\n",
      "finished 2648\n",
      "finished 2656\n",
      "finished 2664\n",
      "finished 2672\n",
      "finished 2680\n",
      "finished 2688\n",
      "finished 2696\n",
      "finished 2704\n",
      "finished 2712\n",
      "finished 2720\n",
      "finished 2728\n",
      "finished 2736\n",
      "finished 2744\n",
      "finished 2752\n",
      "finished 2760\n",
      "finished 2768\n",
      "finished 2776\n",
      "finished 2784\n",
      "finished 2792\n",
      "finished 2800\n",
      "finished 2808\n",
      "finished 2816\n",
      "finished 2824\n",
      "finished 2832\n",
      "finished 2840\n",
      "finished 2848\n",
      "finished 2856\n",
      "finished 2864\n",
      "finished 2872\n",
      "finished 2880\n",
      "finished 2888\n",
      "finished 2896\n",
      "finished 2904\n",
      "finished 2912\n",
      "finished 2920\n",
      "finished 2928\n",
      "finished 2936\n",
      "finished 2944\n",
      "finished 2952\n",
      "finished 2960\n",
      "finished 2968\n",
      "finished 2976\n",
      "finished 2984\n",
      "finished 2992\n",
      "finished 3000\n",
      "finished 3008\n",
      "finished 3016\n",
      "finished 3024\n",
      "finished 3032\n",
      "finished 3040\n",
      "finished 3048\n",
      "finished 3056\n",
      "finished 3064\n",
      "finished 3072\n",
      "finished 3080\n",
      "finished 3088\n",
      "finished 3096\n",
      "finished 3104\n",
      "finished 3112\n",
      "finished 3120\n",
      "finished 3128\n",
      "finished 3136\n",
      "finished 3144\n",
      "finished 3152\n",
      "finished 3160\n",
      "finished 3168\n",
      "finished 3176\n",
      "finished 3184\n",
      "finished 3192\n",
      "finished 3200\n",
      "finished 3208\n",
      "finished 3216\n",
      "finished 3224\n",
      "finished 3232\n",
      "finished 3240\n",
      "finished 3248\n",
      "finished 3256\n",
      "finished 3264\n",
      "finished 3272\n",
      "finished 3280\n",
      "finished 3288\n",
      "finished 3296\n",
      "finished 3304\n",
      "finished 3312\n",
      "finished 3320\n",
      "finished 3328\n",
      "finished 3336\n",
      "finished 3344\n",
      "finished 3352\n",
      "finished 3360\n",
      "finished 3368\n",
      "finished 3376\n",
      "finished 3384\n",
      "finished 3392\n",
      "finished 3400\n",
      "finished 3408\n",
      "finished 3416\n",
      "finished 3424\n",
      "finished 3432\n",
      "finished 3440\n",
      "finished 3448\n",
      "finished 3456\n",
      "finished 3464\n",
      "finished 3472\n",
      "finished 3480\n",
      "finished 3488\n",
      "finished 3496\n",
      "finished 3504\n",
      "finished 3512\n",
      "finished 3520\n",
      "finished 3528\n",
      "finished 3536\n",
      "finished 3544\n",
      "finished 3552\n",
      "finished 3560\n",
      "finished 3568\n",
      "finished 3576\n",
      "finished 3584\n",
      "finished 3592\n",
      "finished 3600\n",
      "finished 3608\n",
      "finished 3616\n",
      "finished 3624\n",
      "finished 3632\n",
      "finished 3640\n",
      "finished 3648\n",
      "finished 3656\n",
      "finished 3664\n",
      "finished 3672\n",
      "finished 3680\n",
      "finished 3688\n",
      "finished 3696\n",
      "finished 3704\n",
      "finished 3712\n",
      "finished 3720\n",
      "finished 3728\n",
      "finished 3736\n",
      "finished 3744\n",
      "finished 3752\n",
      "finished 3760\n",
      "finished 3768\n",
      "finished 3776\n",
      "finished 3784\n",
      "finished 3792\n",
      "finished 3800\n",
      "finished 3808\n",
      "finished 3816\n",
      "finished 3824\n",
      "finished 3832\n",
      "finished 3840\n",
      "finished 3848\n",
      "finished 3856\n",
      "finished 3864\n",
      "finished 3872\n",
      "finished 3880\n",
      "finished 3888\n",
      "finished 3896\n",
      "finished 3904\n",
      "finished 3912\n",
      "finished 3920\n",
      "finished 3928\n",
      "finished 3936\n",
      "finished 3944\n",
      "finished 3952\n",
      "finished 3960\n",
      "finished 3968\n",
      "finished 3976\n",
      "finished 3984\n",
      "finished 3992\n",
      "finished 4000\n",
      "finished 4008\n",
      "finished 4016\n",
      "finished 4024\n",
      "finished 4032\n",
      "finished 4040\n",
      "finished 4048\n",
      "finished 4056\n",
      "finished 4064\n",
      "finished 4072\n",
      "finished 4080\n",
      "finished 4088\n",
      "finished 4096\n",
      "finished 4104\n",
      "finished 4112\n",
      "finished 4120\n",
      "finished 4128\n",
      "finished 4136\n",
      "finished 4144\n",
      "finished 4152\n",
      "finished 4160\n",
      "finished 4168\n",
      "finished 4176\n",
      "finished 4184\n",
      "finished 4192\n",
      "finished 4200\n",
      "finished 4208\n",
      "finished 4216\n",
      "finished 4224\n",
      "finished 4232\n",
      "finished 4240\n",
      "finished 4248\n",
      "finished 4256\n",
      "finished 4264\n",
      "finished 4272\n",
      "finished 4280\n",
      "finished 4288\n",
      "finished 4296\n",
      "finished 4304\n",
      "finished 4312\n",
      "finished 4320\n",
      "finished 4328\n",
      "finished 4336\n",
      "finished 4344\n",
      "finished 4352\n",
      "finished 4360\n",
      "finished 4368\n",
      "finished 4376\n",
      "finished 4384\n",
      "finished 4392\n",
      "finished 4400\n",
      "finished 4408\n",
      "finished 4416\n",
      "finished 4424\n",
      "finished 4432\n",
      "finished 4440\n",
      "finished 4448\n",
      "finished 4456\n",
      "finished 4464\n",
      "finished 4472\n",
      "finished 4480\n",
      "finished 4488\n",
      "finished 4496\n",
      "finished 4504\n",
      "finished 4512\n",
      "finished 4520\n",
      "finished 4528\n",
      "finished 4536\n",
      "finished 4544\n",
      "finished 4552\n",
      "finished 4560\n",
      "finished 4568\n",
      "finished 4576\n",
      "finished 4584\n",
      "finished 4592\n",
      "finished 4600\n",
      "finished 4608\n",
      "finished 4616\n",
      "finished 4624\n",
      "finished 4632\n",
      "finished 4640\n",
      "finished 4648\n",
      "finished 4656\n",
      "finished 4664\n",
      "finished 4672\n",
      "finished 4680\n",
      "finished 4688\n",
      "finished 4696\n",
      "finished 4704\n",
      "finished 4712\n",
      "finished 4720\n",
      "finished 4728\n",
      "finished 4736\n",
      "finished 4744\n",
      "finished 4752\n",
      "finished 4760\n",
      "finished 4768\n",
      "finished 4776\n",
      "finished 4784\n",
      "finished 4792\n",
      "finished 4800\n",
      "finished 4808\n",
      "finished 4816\n",
      "finished 4824\n",
      "finished 4832\n",
      "finished 4840\n",
      "finished 4848\n",
      "finished 4856\n",
      "finished 4864\n",
      "finished 4872\n",
      "finished 4880\n",
      "finished 4888\n",
      "finished 4896\n",
      "finished 4904\n",
      "finished 4912\n",
      "finished 4920\n",
      "finished 4928\n",
      "finished 4936\n",
      "finished 4944\n",
      "finished 4952\n",
      "finished 4960\n",
      "finished 4968\n",
      "finished 4976\n",
      "finished 4984\n",
      "finished 4992\n",
      "finished 5000\n",
      "finished 5008\n",
      "finished 5016\n",
      "finished 5024\n",
      "finished 5032\n",
      "finished 5040\n",
      "finished 5048\n",
      "finished 5056\n",
      "finished 5064\n",
      "finished 5072\n",
      "finished 5080\n",
      "finished 5088\n",
      "finished 5096\n",
      "finished 5104\n",
      "finished 5112\n",
      "finished 5120\n",
      "finished 5128\n",
      "finished 5136\n",
      "finished 5144\n",
      "finished 5152\n",
      "finished 5160\n",
      "finished 5168\n",
      "finished 5176\n",
      "finished 5184\n",
      "finished 5192\n",
      "finished 5200\n",
      "finished 5208\n",
      "finished 5216\n",
      "finished 5224\n",
      "finished 5232\n",
      "finished 5240\n",
      "finished 5248\n",
      "finished 5256\n",
      "finished 5264\n",
      "finished 5272\n",
      "finished 5280\n",
      "finished 5288\n",
      "finished 5296\n",
      "finished 5304\n",
      "finished 5312\n",
      "finished 5320\n",
      "finished 5328\n",
      "finished 5336\n",
      "finished 5344\n",
      "finished 5352\n",
      "finished 5360\n",
      "finished 5368\n",
      "finished 5376\n",
      "finished 5384\n",
      "Train:\n",
      "  train_acc = 0.4952593418851088, train_loss = 4.80593297656011\n",
      "finished 5392\n",
      "finished 5400\n",
      "finished 5408\n",
      "finished 5416\n",
      "finished 5424\n",
      "finished 5432\n",
      "finished 5440\n",
      "finished 5448\n",
      "finished 5456\n",
      "finished 5464\n",
      "finished 5472\n",
      "finished 5480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[138], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  train_acc = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, train_loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m t \u001B[38;5;241m%\u001B[39m VAL_FREQ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m t \u001B[38;5;241m==\u001B[39m model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 21\u001B[0m     val_acc, val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mval_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutfit_classifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     train_data\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphase\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     24\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m: t,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: val_loss\n\u001B[1;32m     28\u001B[0m     })\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    val_acc = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val_loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[136], line 14\u001B[0m, in \u001B[0;36mval_loop\u001B[0;34m(dataloader, feature_model, loss_fn)\u001B[0m\n\u001B[1;32m     12\u001B[0m X \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m     13\u001B[0m y \u001B[38;5;241m=\u001B[39m y\n\u001B[0;32m---> 14\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mfeature_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m val_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     16\u001B[0m val_acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (pred\u001B[38;5;241m.\u001B[39mround() \u001B[38;5;241m==\u001B[39m y)\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mfloat)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[117], line 90\u001B[0m, in \u001B[0;36mOutfitClassifier.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(number_of_rows):\n\u001B[1;32m     88\u001B[0m         extracted_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(\n\u001B[1;32m     89\u001B[0m             (\n\u001B[0;32m---> 90\u001B[0m                 create_linear_input(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accessoire_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m, model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_n_blocks\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     91\u001B[0m                                     \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     92\u001B[0m                 create_linear_input(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_wear_model\u001B[38;5;241m.\u001B[39mforward(X[\u001B[38;5;241m1\u001B[39m][index]), model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_n_blocks\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     93\u001B[0m                                     \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     94\u001B[0m                 create_linear_input(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outer_wear_model\u001B[38;5;241m.\u001B[39mforward(X[\u001B[38;5;241m2\u001B[39m][index]), model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_n_blocks\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     95\u001B[0m                                     \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     96\u001B[0m                 create_linear_input(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bottom_wear_model\u001B[38;5;241m.\u001B[39mforward(X[\u001B[38;5;241m3\u001B[39m][index]), model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_n_blocks\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     97\u001B[0m                                     \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     98\u001B[0m                 create_linear_input(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shoes_model\u001B[38;5;241m.\u001B[39mforward(X[\u001B[38;5;241m4\u001B[39m][index]), model_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_n_blocks\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     99\u001B[0m             ),\n\u001B[1;32m    100\u001B[0m             dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    101\u001B[0m         )\n\u001B[1;32m    102\u001B[0m         extracted_features\u001B[38;5;241m.\u001B[39mrequires_grad_()\n\u001B[1;32m    104\u001B[0m         feature_list\u001B[38;5;241m.\u001B[39mappend(extracted_features)\n",
      "Cell \u001B[0;32mIn[117], line 18\u001B[0m, in \u001B[0;36mModelWithIntermediateLayers.forward\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[0;32m---> 18\u001B[0m         features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_intermediate_layers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_last_blocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_class_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m features\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:309\u001B[0m, in \u001B[0;36mDinoVisionTransformer.get_intermediate_layers\u001B[0;34m(self, x, n, reshape, return_class_token, norm)\u001B[0m\n\u001B[1;32m    307\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_intermediate_layers_chunked(x, n)\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 309\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_intermediate_layers_not_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m norm:\n\u001B[1;32m    311\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(out) \u001B[38;5;28;01mfor\u001B[39;00m out \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:278\u001B[0m, in \u001B[0;36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001B[0;34m(self, x, n)\u001B[0m\n\u001B[1;32m    276\u001B[0m blocks_to_take \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(total_block_len \u001B[38;5;241m-\u001B[39m n, total_block_len) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m n\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks):\n\u001B[0;32m--> 278\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m blocks_to_take:\n\u001B[1;32m    280\u001B[0m         output\u001B[38;5;241m.\u001B[39mappend(x)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:254\u001B[0m, in \u001B[0;36mNestedTensorBlock.forward\u001B[0;34m(self, x_or_x_list)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_or_x_list):\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_or_x_list, Tensor):\n\u001B[0;32m--> 254\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_or_x_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_or_x_list, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m XFORMERS_AVAILABLE:\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:112\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    110\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path1(ffn_residual_func(x))  \u001B[38;5;66;03m# FIXME: drop_path2\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 112\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[43mattn_residual_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m ffn_residual_func(x)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:91\u001B[0m, in \u001B[0;36mBlock.forward.<locals>.attn_residual_func\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattn_residual_func\u001B[39m(x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mls1(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/DSPRO2-outfit-recommendation-QOdjBNZs/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:77\u001B[0m, in \u001B[0;36mMemEffAttention.forward\u001B[0;34m(self, x, attn_bias)\u001B[0m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attn_bias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxFormers is required for using nested tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m B, N, C \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m     80\u001B[0m qkv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqkv(x)\u001B[38;5;241m.\u001B[39mreshape(B, N, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, C \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads)\n",
      "File \u001B[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     63\u001B[0m attn \u001B[38;5;241m=\u001B[39m attn\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     64\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_drop(attn)\n\u001B[0;32m---> 66\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreshape(B, N, C)\n\u001B[1;32m     67\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(x)\n\u001B[1;32m     68\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_drop(x)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "VAL_FREQ = 1\n",
    "best_acc = 0.0\n",
    "best_acc_loss = np.inf\n",
    "train_data = []\n",
    "for t in range(model_config['epochs']):\n",
    "    print(f'Epoch {t + 1}\\n-------------------------------')\n",
    "\n",
    "    train_acc, train_loss = train_loop(dataloaders['train'], outfit_classifier, loss_fn, optimizer)\n",
    "    train_data.append({\n",
    "        'phase': 'train',\n",
    "        'epoch': t,\n",
    "        'lr': optimizer.param_groups[0][\"lr\"],\n",
    "        'accuracy': train_acc,\n",
    "        'loss': train_loss\n",
    "    })\n",
    "    scheduler.step()\n",
    "    print(f'Train:\\n  train_acc = {train_acc}, train_loss = {train_loss}')\n",
    "    if t % VAL_FREQ == 0 or t == model_config['epochs'] - 1:\n",
    "        val_acc, val_loss = val_loop(dataloaders['val'], outfit_classifier, loss_fn)\n",
    "        train_data.append({\n",
    "            'phase': 'val',\n",
    "            'epoch': t,\n",
    "            'lr': optimizer.param_groups[0][\"lr\"],\n",
    "            'accuracy': val_acc,\n",
    "            'loss': val_loss\n",
    "        })\n",
    "        print(f'Validation:\\n    val_acc = {val_acc}, val_loss = {val_loss}')\n",
    "        if (val_acc == best_acc and val_loss < best_acc_loss) or (val_acc > best_acc):\n",
    "            best_acc, best_acc_loss = val_acc, val_loss\n",
    "            print(f'Best accuracy = {best_acc}, best_loss = {best_acc_loss}')\n",
    "            save_dict = {\n",
    "                'epoch': t + 1,\n",
    "                'state_dict': outfit_classifier.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                # 'scheduler': scheduler.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'best_loss': best_acc_loss\n",
    "            }\n",
    "            torch.save(save_dict, os.path.join('.', 'dino_classifier_ckpt.pth'))\n",
    "\n",
    "    print('\\n')\n",
    "print('Training completed.')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
